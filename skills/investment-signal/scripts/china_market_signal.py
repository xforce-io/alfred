#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Aè‚¡/æ¸¯è‚¡å¸‚åœºä¿¡å·åˆ†æå™¨ (China & HK Market Signal Analyzer)

å››ç»´åº¦ç›‘æ§:

| ç»´åº¦       | æ•°æ®æº                    | é¢„è­¦æ¡ä»¶                       | æƒé‡ |
|-----------|--------------------------|-------------------------------|------|
| åŒ—å‘èµ„é‡‘   | Tushare moneyflow_hsgt   | å•æ—¥å‡€æµå‡º>50äº¿ æˆ– è¿ç»­3æ—¥å‡€æµå‡º  | 30%  |
| ä¸¤å¸‚æˆäº¤é¢ | Tushare index_daily      | ä½äº8000äº¿(å†·) / è¶…2ä¸‡äº¿(çƒ­)    | 25%  |
| èèµ„èåˆ¸   | Tushare margin            | ä½™é¢å‘¨å˜åŒ–>5% æˆ– æ€¥é™           | 25%  |
| å—å‘èµ„é‡‘   | Tushare moneyflow_hsgt   | å•æ—¥å‡€æµå‡º>30äº¿ æˆ– è¿ç»­3æ—¥å‡€æµå‡º  | 20%  |

çŠ¶æ€è¾“å‡º: ç§¯æ(Bullish) / ä¸­æ€§(Neutral) / è°¨æ…(Cautious) / é˜²å¾¡(Defensive)

Usage:
    python china_market_signal.py [--lookback-days 60] [--format json|text]

Environment:
    TUSHARE_TOKEN - Required. Get token at https://tushare.pro/register
"""

import os
import sys
import json
import logging
import argparse
from typing import Dict, Any
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


# Check dependencies
def _check_dependencies():
    missing = []
    try:
        import pandas
    except ImportError:
        missing.append('pandas')
    try:
        import numpy
    except ImportError:
        missing.append('numpy')
    try:
        import tushare
    except ImportError:
        missing.append('tushare')
    if missing:
        print(f"Missing dependencies: {', '.join(missing)}", file=sys.stderr)
        print(f"Install with: pip install {' '.join(missing)}", file=sys.stderr)
        sys.exit(1)


_check_dependencies()

import numpy as np
import pandas as pd

# é˜ˆå€¼é…ç½®
THRESHOLDS = {
    'north_outflow_single_day': -50,   # åŒ—å‘å•æ—¥å‡€æµå‡ºè¶…50äº¿(ç™¾ä¸‡å…ƒ â†’ -5000)
    'north_consecutive_outflow': 3,     # åŒ—å‘è¿ç»­æµå‡ºå¤©æ•°
    'south_outflow_single_day': -30,   # å—å‘å•æ—¥å‡€æµå‡ºè¶…30äº¿
    'south_consecutive_outflow': 3,
    'turnover_cold': 8000,              # ä¸¤å¸‚æˆäº¤é¢å†·æ·¡çº¿(äº¿å…ƒ)
    'turnover_hot': 20000,              # ä¸¤å¸‚æˆäº¤é¢è¿‡çƒ­çº¿(äº¿å…ƒ)
    'margin_weekly_change_pct': 5.0,    # èèµ„ä½™é¢å‘¨å˜åŒ–è¶…5%
}

DIMENSION_WEIGHTS = {
    'northbound': 0.30,
    'turnover': 0.25,
    'margin': 0.25,
    'southbound': 0.20,
}

STATUS_MAP = [
    (75, 'Defensive', 'é˜²å¾¡', 'ğŸ”´'),
    (50, 'Cautious', 'è°¨æ…', 'ğŸŸ '),
    (25, 'Neutral', 'ä¸­æ€§', 'ğŸŸ¡'),
    (0, 'Bullish', 'ç§¯æ', 'ğŸŸ¢'),
]


def _get_pro():
    """Get tushare pro API instance"""
    import tushare as ts
    token = os.environ.get('TUSHARE_TOKEN')
    if not token:
        raise ValueError(
            "éœ€è¦ TUSHARE_TOKENï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ã€‚"
            "æ³¨å†Œè·å–: https://tushare.pro/register"
        )
    return ts.pro_api(token)


class ChinaMarketSignalAnalyzer:
    """
    Aè‚¡/æ¸¯è‚¡å¸‚åœºä¿¡å·åˆ†æå™¨

    ç›‘æ§åŒ—å‘èµ„é‡‘ã€ä¸¤å¸‚æˆäº¤é¢ã€èèµ„èåˆ¸ã€å—å‘èµ„é‡‘ï¼Œå¤šæ¡ä»¶è§¦å‘é¢„è­¦ã€‚
    """

    def __init__(self):
        self._pro = None

    def _get_pro(self):
        """æ‡’åŠ è½½ tushare pro API"""
        if self._pro is None:
            self._pro = _get_pro()
        return self._pro

    def analyze(self, lookback_days: int = 60) -> Dict[str, Any]:
        """
        å…¨é‡åˆ†æ A è‚¡/æ¸¯è‚¡å¸‚åœºä¿¡å·

        Args:
            lookback_days: å›æº¯å¤©æ•°

        Returns:
            åˆ†æç»“æœ
        """
        signals = []
        dimension_scores = {}
        dimensions = {}

        # 1. åŒ—å‘èµ„é‡‘
        try:
            nb_result = self._fetch_northbound(lookback_days)
            dimensions['northbound'] = nb_result
            dimension_scores['northbound'] = nb_result.get('risk_score', 50)
            signals.extend(nb_result.get('signals', []))
        except Exception as e:
            logger.error(f"è·å–åŒ—å‘èµ„é‡‘æ•°æ®å¤±è´¥: {e}")
            dimensions['northbound'] = {'error': str(e)}
            dimension_scores['northbound'] = 50

        # 2. ä¸¤å¸‚æˆäº¤é¢
        try:
            tv_result = self._fetch_turnover(lookback_days)
            dimensions['turnover'] = tv_result
            dimension_scores['turnover'] = tv_result.get('risk_score', 50)
            signals.extend(tv_result.get('signals', []))
        except Exception as e:
            logger.error(f"è·å–æˆäº¤é¢æ•°æ®å¤±è´¥: {e}")
            dimensions['turnover'] = {'error': str(e)}
            dimension_scores['turnover'] = 50

        # 3. èèµ„èåˆ¸
        try:
            mg_result = self._fetch_margin(lookback_days)
            dimensions['margin'] = mg_result
            dimension_scores['margin'] = mg_result.get('risk_score', 50)
            signals.extend(mg_result.get('signals', []))
        except Exception as e:
            logger.error(f"è·å–èèµ„èåˆ¸æ•°æ®å¤±è´¥: {e}")
            dimensions['margin'] = {'error': str(e)}
            dimension_scores['margin'] = 50

        # 4. å—å‘èµ„é‡‘
        try:
            sb_result = self._fetch_southbound(lookback_days)
            dimensions['southbound'] = sb_result
            dimension_scores['southbound'] = sb_result.get('risk_score', 50)
            signals.extend(sb_result.get('signals', []))
        except Exception as e:
            logger.error(f"è·å–å—å‘èµ„é‡‘æ•°æ®å¤±è´¥: {e}")
            dimensions['southbound'] = {'error': str(e)}
            dimension_scores['southbound'] = 50

        # åŠ æƒé£é™©åˆ†æ•°
        total_risk_score = sum(
            dimension_scores.get(dim, 50) * weight
            for dim, weight in DIMENSION_WEIGHTS.items()
        )
        total_risk_score = round(total_risk_score, 1)

        # çŠ¶æ€åˆ¤å®š
        status_en, status_cn, status_icon = 'Neutral', 'ä¸­æ€§', 'ğŸŸ¡'
        for threshold, en, cn, icon in STATUS_MAP:
            if total_risk_score >= threshold:
                status_en, status_cn, status_icon = en, cn, icon
                break

        return {
            'status': status_en,
            'status_cn': status_cn,
            'status_icon': status_icon,
            'risk_score': total_risk_score,
            'dimensions': dimensions,
            'dimension_scores': dimension_scores,
            'signals': signals,
            'thresholds': THRESHOLDS,
            'analyzed_at': datetime.now().isoformat(),
        }

    # ==================== æ•°æ®è·å– ====================

    def _fetch_hsgt_flow(self, lookback_days: int = 60) -> pd.DataFrame:
        """è·å–äº’è”äº’é€šèµ„é‡‘æµå‘æ•°æ® (moneyflow_hsgt)"""
        pro = self._get_pro()
        end = datetime.now().strftime('%Y%m%d')
        start = (datetime.now() - timedelta(days=lookback_days + 10)).strftime('%Y%m%d')

        df = pro.moneyflow_hsgt(start_date=start, end_date=end)

        if df is None or df.empty:
            return pd.DataFrame()

        df['trade_date'] = pd.to_datetime(df['trade_date'])
        df = df.set_index('trade_date').sort_index()
        return df

    def _fetch_northbound(self, lookback_days: int = 60) -> Dict[str, Any]:
        """
        è·å–åŒ—å‘èµ„é‡‘æ•°æ® (æ²ªè‚¡é€š+æ·±è‚¡é€š)

        åŒ—å‘ = north_money æˆ– hgt + sgt
        """
        df = self._fetch_hsgt_flow(lookback_days)

        if df.empty:
            return {'error': 'åŒ—å‘èµ„é‡‘æ•°æ®ä¸ºç©º', 'risk_score': 50}

        # è®¡ç®—åŒ—å‘å‡€æµå…¥ (ç™¾ä¸‡å…ƒ)
        north_net = None
        if 'north_money' in df.columns:
            north_net = df['north_money']
        elif 'hgt' in df.columns and 'sgt' in df.columns:
            north_net = df['hgt'] + df['sgt']

        if north_net is None or north_net.empty:
            return {'error': 'æ— æ³•è®¡ç®—åŒ—å‘å‡€æµå…¥', 'risk_score': 50}

        # è½¬ä¸ºäº¿å…ƒ
        north_net_yi = north_net / 100  # ç™¾ä¸‡â†’äº¿

        latest = float(north_net_yi.iloc[-1]) if len(north_net_yi) > 0 else 0
        recent_5 = north_net_yi.tail(5)
        recent_5_sum = float(recent_5.sum())
        consecutive_outflow = 0
        for val in reversed(north_net_yi.values):
            if val < 0:
                consecutive_outflow += 1
            else:
                break

        # é£é™©è¯„åˆ†
        risk_score = 0
        signals = []

        if latest < THRESHOLDS['north_outflow_single_day']:
            risk_score += 50
            signals.append(f'ğŸ”´ åŒ—å‘å•æ—¥å¤§å¹…æµå‡º {latest:.1f}äº¿ï¼ˆé˜ˆå€¼ {THRESHOLDS["north_outflow_single_day"]}äº¿ï¼‰')
        elif latest < -20:
            risk_score += 30
            signals.append(f'ğŸŸ  åŒ—å‘å‡€æµå‡º {latest:.1f}äº¿')
        elif latest > 50:
            signals.append(f'ğŸŸ¢ åŒ—å‘å¤§å¹…å‡€æµå…¥ {latest:.1f}äº¿')
        else:
            signals.append(f'ğŸŸ¢ åŒ—å‘èµ„é‡‘ {latest:+.1f}äº¿')

        if consecutive_outflow >= THRESHOLDS['north_consecutive_outflow']:
            risk_score += 40
            signals.append(f'ğŸ”´ åŒ—å‘è¿ç»­ {consecutive_outflow} æ—¥å‡€æµå‡º')
        elif consecutive_outflow >= 2:
            risk_score += 15

        risk_score = min(risk_score, 100)

        return {
            'latest': round(latest, 1),
            'recent_5d_sum': round(recent_5_sum, 1),
            'consecutive_outflow': consecutive_outflow,
            'risk_score': risk_score,
            'signals': signals,
        }

    def _fetch_turnover(self, lookback_days: int = 60) -> Dict[str, Any]:
        """
        è·å–ä¸¤å¸‚æˆäº¤é¢

        ä½¿ç”¨ä¸Šè¯æŒ‡æ•°+æ·±è¯æˆæŒ‡çš„æˆäº¤é¢åˆè®¡
        """
        pro = self._get_pro()
        end = datetime.now().strftime('%Y%m%d')
        start = (datetime.now() - timedelta(days=lookback_days + 10)).strftime('%Y%m%d')

        # è·å–ä¸Šè¯å’Œæ·±è¯æŒ‡æ•°æ•°æ®
        try:
            sh_df = pro.index_daily(ts_code='000001.SH', start_date=start, end_date=end,
                                    fields='trade_date,amount')
            sz_df = pro.index_daily(ts_code='399001.SZ', start_date=start, end_date=end,
                                    fields='trade_date,amount')
        except Exception as e:
            return {'error': f'è·å–æŒ‡æ•°æ•°æ®å¤±è´¥: {e}', 'risk_score': 50}

        if sh_df is None or sh_df.empty or sz_df is None or sz_df.empty:
            return {'error': 'æŒ‡æ•°æˆäº¤é¢æ•°æ®ä¸ºç©º', 'risk_score': 50}

        sh_df['trade_date'] = pd.to_datetime(sh_df['trade_date'])
        sz_df['trade_date'] = pd.to_datetime(sz_df['trade_date'])

        merged = pd.merge(sh_df, sz_df, on='trade_date', suffixes=('_sh', '_sz'))
        merged = merged.sort_values('trade_date').set_index('trade_date')

        # amount å•ä½: åƒå…ƒ â†’ äº¿å…ƒ
        merged['total_amount'] = (merged['amount_sh'] + merged['amount_sz']) / 1e5

        latest = float(merged['total_amount'].iloc[-1])
        avg_20 = float(merged['total_amount'].tail(20).mean())
        avg_5 = float(merged['total_amount'].tail(5).mean())

        # é£é™©è¯„åˆ† (æˆäº¤ä½è¿· = é«˜é£é™©)
        risk_score = 0
        signals = []

        if latest < THRESHOLDS['turnover_cold']:
            risk_score = 70
            signals.append(f'ğŸ”´ ä¸¤å¸‚æˆäº¤é¢èç¼©è‡³ {latest:.0f}äº¿ï¼ˆä½äº{THRESHOLDS["turnover_cold"]}äº¿å†·æ·¡çº¿ï¼‰')
        elif latest < 10000:
            risk_score = 40
            signals.append(f'ğŸŸ  ä¸¤å¸‚æˆäº¤é¢åä½ {latest:.0f}äº¿')
        elif latest > THRESHOLDS['turnover_hot']:
            risk_score = 30  # è¿‡çƒ­ä¹Ÿæœ‰é£é™©
            signals.append(f'ğŸŸ  ä¸¤å¸‚æˆäº¤é¢è¿‡çƒ­ {latest:.0f}äº¿ï¼ˆè¶…{THRESHOLDS["turnover_hot"]}äº¿ï¼‰')
        else:
            risk_score = 10
            signals.append(f'ğŸŸ¢ ä¸¤å¸‚æˆäº¤é¢ {latest:.0f}äº¿ï¼ˆæ­£å¸¸ï¼‰')

        return {
            'latest': round(latest, 0),
            'avg_5d': round(avg_5, 0),
            'avg_20d': round(avg_20, 0),
            'risk_score': risk_score,
            'signals': signals,
        }

    def _fetch_margin(self, lookback_days: int = 60) -> Dict[str, Any]:
        """
        è·å–èèµ„èåˆ¸æ•°æ®

        Tushare: margin (exchange_id='SSE'/'SZSE')
        """
        pro = self._get_pro()
        end = datetime.now().strftime('%Y%m%d')
        start = (datetime.now() - timedelta(days=lookback_days + 10)).strftime('%Y%m%d')

        try:
            # ä¸Šäº¤æ‰€ + æ·±äº¤æ‰€
            sse = pro.margin(exchange_id='SSE', start_date=start, end_date=end)
            szse = pro.margin(exchange_id='SZSE', start_date=start, end_date=end)
        except Exception as e:
            return {'error': f'è·å–èèµ„èåˆ¸æ•°æ®å¤±è´¥: {e}', 'risk_score': 50}

        frames = []
        for df in [sse, szse]:
            if df is not None and not df.empty:
                frames.append(df)

        if not frames:
            return {'error': 'èèµ„èåˆ¸æ•°æ®ä¸ºç©º', 'risk_score': 50}

        combined = pd.concat(frames)
        combined['trade_date'] = pd.to_datetime(combined['trade_date'])

        # æŒ‰æ—¥æœŸæ±‡æ€» (rzye=èèµ„ä½™é¢, rqye=èåˆ¸ä½™é¢, rzmre=èèµ„ä¹°å…¥é¢)
        daily = combined.groupby('trade_date').agg({
            'rzye': 'sum',    # èèµ„ä½™é¢
            'rqye': 'sum',    # èåˆ¸ä½™é¢
            'rzmre': 'sum',   # èèµ„ä¹°å…¥é¢
        }).sort_index()

        # è½¬ä¸ºäº¿å…ƒ
        daily['rzye_yi'] = daily['rzye'] / 1e8
        daily['rqye_yi'] = daily['rqye'] / 1e8
        daily['total_yi'] = daily['rzye_yi'] + daily['rqye_yi']

        latest_rz = float(daily['rzye_yi'].iloc[-1])
        latest_total = float(daily['total_yi'].iloc[-1])

        # å‘¨å˜åŒ–
        if len(daily) >= 5:
            week_ago = float(daily['rzye_yi'].iloc[-5])
            weekly_change_pct = (latest_rz - week_ago) / week_ago * 100 if week_ago != 0 else 0
        else:
            weekly_change_pct = 0

        # é£é™©è¯„åˆ†
        risk_score = 0
        signals = []

        if weekly_change_pct > THRESHOLDS['margin_weekly_change_pct']:
            risk_score = 40  # æ æ†æ€¥å¢ï¼Œæœ‰è¿‡çƒ­é£é™©
            signals.append(f'ğŸŸ  èèµ„ä½™é¢å‘¨å¢ {weekly_change_pct:.1f}%ï¼ˆæ æ†ä¸Šè¡Œï¼‰')
        elif weekly_change_pct < -THRESHOLDS['margin_weekly_change_pct']:
            risk_score = 70  # æ æ†æ€¥é™ï¼Œææ…Œ
            signals.append(f'ğŸ”´ èèµ„ä½™é¢å‘¨é™ {weekly_change_pct:.1f}%ï¼ˆå»æ æ†ä¿¡å·ï¼‰')
        elif weekly_change_pct < -2:
            risk_score = 40
            signals.append(f'ğŸŸ  èèµ„ä½™é¢å°å¹…ä¸‹é™ {weekly_change_pct:.1f}%')
        else:
            risk_score = 10
            signals.append(f'ğŸŸ¢ èèµ„ä½™é¢ç¨³å®š {latest_rz:.0f}äº¿ (å‘¨å˜åŒ–{weekly_change_pct:+.1f}%)')

        return {
            'rz_balance': round(latest_rz, 0),
            'total_balance': round(latest_total, 0),
            'weekly_change_pct': round(weekly_change_pct, 2),
            'risk_score': risk_score,
            'signals': signals,
        }

    def _fetch_southbound(self, lookback_days: int = 60) -> Dict[str, Any]:
        """
        è·å–å—å‘èµ„é‡‘æ•°æ® (æ¸¯è‚¡é€š: sh2hk + sz2hk)
        """
        df = self._fetch_hsgt_flow(lookback_days)

        if df.empty:
            return {'error': 'å—å‘èµ„é‡‘æ•°æ®ä¸ºç©º', 'risk_score': 50}

        # å—å‘å‡€æµå…¥ (ç™¾ä¸‡å…ƒ)
        south_net = None
        if 'south_money' in df.columns:
            south_net = df['south_money']
        elif 'hgt' in df.columns:
            # moneyflow_hsgt returns: ggt_ss (æ¸¯è‚¡é€šä¸Šæµ·), ggt_sz (æ¸¯è‚¡é€šæ·±åœ³)
            if 'ggt_ss' in df.columns and 'ggt_sz' in df.columns:
                south_net = df['ggt_ss'] + df['ggt_sz']

        if south_net is None or south_net.empty:
            return {'error': 'æ— æ³•è®¡ç®—å—å‘å‡€æµå…¥', 'risk_score': 50}

        south_net_yi = south_net / 100  # ç™¾ä¸‡â†’äº¿

        latest = float(south_net_yi.iloc[-1]) if len(south_net_yi) > 0 else 0
        recent_5_sum = float(south_net_yi.tail(5).sum())
        consecutive_outflow = 0
        for val in reversed(south_net_yi.values):
            if val < 0:
                consecutive_outflow += 1
            else:
                break

        # é£é™©è¯„åˆ†
        risk_score = 0
        signals = []

        if latest < THRESHOLDS['south_outflow_single_day']:
            risk_score += 50
            signals.append(f'ğŸ”´ å—å‘å•æ—¥å¤§å¹…æµå‡º {latest:.1f}äº¿')
        elif latest < -10:
            risk_score += 25
            signals.append(f'ğŸŸ  å—å‘å‡€æµå‡º {latest:.1f}äº¿')
        elif latest > 30:
            signals.append(f'ğŸŸ¢ å—å‘å¤§å¹…å‡€æµå…¥ {latest:.1f}äº¿')
        else:
            signals.append(f'ğŸŸ¢ å—å‘èµ„é‡‘ {latest:+.1f}äº¿')

        if consecutive_outflow >= THRESHOLDS['south_consecutive_outflow']:
            risk_score += 40
            signals.append(f'ğŸ”´ å—å‘è¿ç»­ {consecutive_outflow} æ—¥å‡€æµå‡º')
        elif consecutive_outflow >= 2:
            risk_score += 15

        risk_score = min(risk_score, 100)

        return {
            'latest': round(latest, 1),
            'recent_5d_sum': round(recent_5_sum, 1),
            'consecutive_outflow': consecutive_outflow,
            'risk_score': risk_score,
            'signals': signals,
        }


def _serialize_result(result: Dict[str, Any]) -> Dict[str, Any]:
    """Remove non-serializable fields for JSON output"""
    cleaned = {}
    for k, v in result.items():
        if isinstance(v, dict):
            cleaned[k] = _serialize_result(v)
        elif isinstance(v, (list, tuple)):
            cleaned[k] = [
                _serialize_result(item) if isinstance(item, dict) else item
                for item in v
            ]
        elif isinstance(v, (np.integer,)):
            cleaned[k] = int(v)
        elif isinstance(v, (np.floating,)):
            cleaned[k] = float(v)
        else:
            cleaned[k] = v
    return cleaned


def format_text(result: Dict[str, Any]) -> str:
    """Format result as human-readable text"""
    lines = []
    lines.append(f"{'='*60}")
    lines.append(f"  Aè‚¡/æ¸¯è‚¡å¸‚åœºä¿¡å·ç›‘æ§æŠ¥å‘Š")
    lines.append(f"  {result['analyzed_at'][:19]}")
    lines.append(f"{'='*60}")
    lines.append(f"")
    lines.append(f"  ç»¼åˆçŠ¶æ€: {result['status_icon']} {result['status']} ({result['status_cn']})")
    lines.append(f"  é£é™©è¯„åˆ†: {result['risk_score']}/100")
    lines.append(f"")

    # ç»´åº¦è¯„åˆ†
    lines.append(f"  {'â”€'*50}")
    lines.append(f"  ç»´åº¦è¯„åˆ†:")
    dim_labels = {
        'northbound': 'åŒ—å‘èµ„é‡‘  (30%)',
        'turnover': 'ä¸¤å¸‚æˆäº¤é¢ (25%)',
        'margin': 'èèµ„èåˆ¸  (25%)',
        'southbound': 'å—å‘èµ„é‡‘  (20%)',
    }
    for dim, label in dim_labels.items():
        score = result['dimension_scores'].get(dim, '?')
        dim_data = result['dimensions'].get(dim, {})
        if 'error' in dim_data:
            lines.append(f"    {label}: âš ï¸  {dim_data['error']}")
        else:
            lines.append(f"    {label}: {score}/100")
            if dim == 'northbound' and 'latest' in dim_data:
                lines.append(f"      æœ€æ–°: {dim_data['latest']:+.1f}äº¿ | è¿‘5æ—¥: {dim_data.get('recent_5d_sum', '?'):+.1f}äº¿ | è¿ç»­æµå‡º: {dim_data.get('consecutive_outflow', 0)}æ—¥")
            elif dim == 'turnover' and 'latest' in dim_data:
                lines.append(f"      æœ€æ–°: {dim_data['latest']:.0f}äº¿ | 5æ—¥å‡: {dim_data.get('avg_5d', '?'):.0f}äº¿ | 20æ—¥å‡: {dim_data.get('avg_20d', '?'):.0f}äº¿")
            elif dim == 'margin' and 'rz_balance' in dim_data:
                lines.append(f"      èèµ„ä½™é¢: {dim_data['rz_balance']:.0f}äº¿ | å‘¨å˜åŒ–: {dim_data.get('weekly_change_pct', '?'):+.1f}%")
            elif dim == 'southbound' and 'latest' in dim_data:
                lines.append(f"      æœ€æ–°: {dim_data['latest']:+.1f}äº¿ | è¿‘5æ—¥: {dim_data.get('recent_5d_sum', '?'):+.1f}äº¿ | è¿ç»­æµå‡º: {dim_data.get('consecutive_outflow', 0)}æ—¥")

    # ä¿¡å·
    lines.append(f"")
    lines.append(f"  {'â”€'*50}")
    lines.append(f"  è§¦å‘ä¿¡å·:")
    if result['signals']:
        for sig in result['signals']:
            lines.append(f"    {sig}")
    else:
        lines.append(f"    æ— è§¦å‘ä¿¡å·")

    lines.append(f"")
    lines.append(f"{'='*60}")
    return '\n'.join(lines)


def main():
    parser = argparse.ArgumentParser(
        description='Aè‚¡/æ¸¯è‚¡å¸‚åœºä¿¡å·åˆ†æå™¨ â€” å››ç»´åº¦é£é™©è¯„åˆ†',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='ç¯å¢ƒå˜é‡: TUSHARE_TOKEN (å¿…éœ€)\næ³¨å†Œè·å–: https://tushare.pro/register'
    )
    parser.add_argument('--lookback-days', type=int, default=60, help='å›æº¯å¤©æ•° (é»˜è®¤ 60)')
    parser.add_argument('--format', choices=['json', 'text'], default='text', help='è¾“å‡ºæ ¼å¼ (é»˜è®¤ text)')
    args = parser.parse_args()

    logging.basicConfig(level=logging.WARNING, format='%(levelname)s: %(message)s')

    analyzer = ChinaMarketSignalAnalyzer()
    result = analyzer.analyze(lookback_days=args.lookback_days)

    if args.format == 'json':
        print(json.dumps(_serialize_result(result), ensure_ascii=False, indent=2))
    else:
        print(format_text(_serialize_result(result)))


if __name__ == '__main__':
    main()
